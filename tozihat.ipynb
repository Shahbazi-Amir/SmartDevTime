{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0f21a34",
   "metadata": {},
   "source": [
    "<div style=\"direction: rtl; white-space: pre-wrap; line-height: 1.5;\">\n",
    "خیلی خوبه که می‌خوای بنیادی بفهمی 🌹\n",
    "پس بذار آروم‌آروم و پایه‌ای توضیح بدم:\n",
    "\n",
    "---\n",
    "\n",
    "# 📌 پایه‌ی کار\n",
    "\n",
    "ما داریم **رگرسیون خطی** انجام می‌دیم.\n",
    "یعنی فرض می‌کنیم:\n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_p x_p + \\epsilon\n",
    "$$\n",
    "\n",
    "هدف: پیدا کردن ضرایب $\\beta$ طوری که پیش‌بینی $y$ به داده‌های واقعی نزدیک باشه.\n",
    "\n",
    "---\n",
    "\n",
    "# 📌 مشکل رگرسیون خطی ساده\n",
    "\n",
    "* وقتی فیچرها زیاد یا همبسته باشن → ضرایب ممکنه خیلی نوسانی و بزرگ بشن.\n",
    "* مدل overfit می‌کنه (یعنی روی دیتای آموزش خوب، ولی روی تست بد).\n",
    "* یا بعضی فیچرهای ضعیف و بی‌اثر همچنان داخل مدل می‌مونن.\n",
    "\n",
    "---\n",
    "\n",
    "# 📌 راه‌حل → **Regularization**\n",
    "\n",
    "ایده: بیایم علاوه بر «خطای پیش‌بینی»، خود ضرایب رو هم جریمه کنیم.\n",
    "یعنی به تابع هزینه‌ی رگرسیون یه term اضافه کنیم.\n",
    "\n",
    "## 🔹 Ridge (L2 penalty)\n",
    "\n",
    "هزینه:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{MSE} + \\alpha \\sum_j \\beta_j^2\n",
    "$$\n",
    "\n",
    "* این جریمه باعث می‌شه ضرایب بزرگ، کوچک بشن.\n",
    "* ولی تقریباً هیچ ضریبی دقیقاً صفر نمی‌شه → همه‌ی فیچرها در مدل می‌مونن.\n",
    "* نتیجه: مدل پایدارتر می‌شه، variance کمتر.\n",
    "\n",
    "## 🔹 Lasso (L1 penalty)\n",
    "\n",
    "هزینه:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{MSE} + \\alpha \\sum_j |\\beta_j|\n",
    "$$\n",
    "\n",
    "* این جریمه باعث می‌شه بعضی ضرایب دقیقاً **صفر** بشن.\n",
    "* یعنی خودش انتخاب ویژگی هم انجام می‌ده.\n",
    "* فرق اصلیش با Ridge همینه: Lasso می‌تونه بگه «این فیچر بی‌خاصیت است → حذفش کن».\n",
    "\n",
    "## 🔹 ElasticNet (ترکیب L1 + L2)\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{MSE} + \\alpha_1 \\sum_j |\\beta_j| + \\alpha_2 \\sum_j \\beta_j^2\n",
    "$$\n",
    "\n",
    "* برای وقتی خوبه که فیچرها خیلی همبسته باشن.\n",
    "* ترکیب مزایای Ridge (پایداری) و Lasso (انتخاب ویژگی).\n",
    "\n",
    "---\n",
    "\n",
    "# 📌 الفا (α) چیه؟\n",
    "\n",
    "* ضریب جریمه است.\n",
    "* اگر α خیلی بزرگ باشه → ضرایب خیلی کوچک یا صفر می‌شن → underfit.\n",
    "* اگر α خیلی کوچک باشه → مدل تقریباً مثل رگرسیون خطی ساده می‌شه.\n",
    "* باید یه تعادل پیدا کنیم → معمولاً با **Cross Validation** انتخاب می‌شه (یعنی تست روی چند تقسیم مختلف از داده).\n",
    "\n",
    "---\n",
    "\n",
    "# 📌 Iter (تکرار) چیه؟\n",
    "\n",
    "* الگوریتمی که Lasso/Ridge حل می‌کنن تحلیلی ساده نیست، باید با **تکرار عددی (iterative algorithm)** حل بشه.\n",
    "* `max_iter` یعنی «حداکثر تعداد قدم‌هایی که الگوریتم اجازه داره بره تا به جواب نزدیک بشه».\n",
    "* اگه `max_iter` کم باشه، الگوریتم ممکنه وسط راه گیر کنه → همون **ConvergenceWarning** که دیدی.\n",
    "\n",
    "---\n",
    "\n",
    "# 📌 خلاصه ساده\n",
    "\n",
    "* **Linear Regression**: فقط خطی با MSE، بدون جریمه.\n",
    "* **Ridge**: رگرسیون خطی + جریمه L2 → ضرایب کوچیک‌تر ولی همه حاضر.\n",
    "* **Lasso**: رگرسیون خطی + جریمه L1 → بعضی ضرایب صفر می‌شن (انتخاب ویژگی).\n",
    "* **ElasticNet**: ترکیب L1 و L2 → هم انتخاب ویژگی، هم پایداری.\n",
    "* **α (alpha)**: شدت جریمه.\n",
    "* **Iter (تکرار)**: تعداد گام‌های الگوریتم برای پیدا کردن ضرایب.\n",
    "\n",
    "---\n",
    "\n",
    "👉 حالا تو دوست داری من اینا رو با فرمول ریاضی بیشتر توضیح بدم (جزییات optimization و هندسه L1 vs L2) یا بیشتر با مثال تصویری/شبیه‌سازی عددی (نمودار ضرایب در برابر α)؟\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7566b695",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
